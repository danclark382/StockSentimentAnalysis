{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import pymongo\n",
    "import twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/dclark171/projects/myKeys.json\", 'r') as f:\n",
    "    data = json.load(f)\n",
    "twitter_keys = data['keys']['twitter']\n",
    "host = data['keys']['mongodb']['projects']['info440']['host']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"etfs.json\", 'r') as f:\n",
    "    etfs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oauth_login():\n",
    "    # XXX: Go to  to create an app and get values\n",
    "    # for these credentials that you'll need to provide in place of these\n",
    "    # empty string values that are defined as placeholders.\n",
    "    # See https://developer.twitter.com/en/docs/basics/authentication/overview/oauth\n",
    "    # for more information on Twitter's OAuth implementation.\n",
    "    \n",
    "    CONSUMER_KEY = twitter_keys['CONSUMER_KEY']\n",
    "    CONSUMER_SECRET = twitter_keys['CONSUMER_SECRET']\n",
    "    OAUTH_TOKEN = twitter_keys['OAUTH_TOKEN']\n",
    "    OAUTH_TOKEN_SECRET = twitter_keys['OAUTH_TOKEN_SECRET']\n",
    "    \n",
    "    auth = twitter.oauth.OAuth(OAUTH_TOKEN, OAUTH_TOKEN_SECRET,\n",
    "                               CONSUMER_KEY, CONSUMER_SECRET)\n",
    "    \n",
    "    twitter_api = twitter.Twitter(auth=auth)\n",
    "    return twitter_api\n",
    "\n",
    "def twitter_search(twitter_api, q, max_results=300, **kw):\n",
    "\n",
    "    # See https://developer.twitter.com/en/docs/tweets/search/api-reference/get-search-tweets\n",
    "    # and https://developer.twitter.com/en/docs/tweets/search/guides/standard-operators\n",
    "    # for details on advanced search criteria that may be useful for \n",
    "    # keyword arguments\n",
    "    \n",
    "    # See https://dev.twitter.com/docs/api/1.1/get/search/tweets    \n",
    "    search_results = twitter_api.search.tweets(q=q, count=100, **kw)\n",
    "    \n",
    "    statuses = search_results['statuses']\n",
    "    \n",
    "    # Iterate through batches of results by following the cursor until we\n",
    "    # reach the desired number of results, keeping in mind that OAuth users\n",
    "    # can \"only\" make 180 search queries per 15-minute interval. See\n",
    "    # https://developer.twitter.com/en/docs/basics/rate-limits\n",
    "    # for details. A reasonable number of results is ~1000, although\n",
    "    # that number of results may not exist for all queries.\n",
    "    \n",
    "    # Enforce a reasonable limit\n",
    "    max_results = min(1000, max_results)\n",
    "    \n",
    "    for _ in range(10): # 10*100 = 1000\n",
    "        try:\n",
    "            next_results = search_results['search_metadata']['next_results']\n",
    "        except KeyError as e: # No more results when next_results doesn't exist\n",
    "            break\n",
    "            \n",
    "        # Create a dictionary from next_results, which has the following form:\n",
    "        # ?max_id=313519052523986943&q=NCAA&include_entities=1\n",
    "        kwargs = dict([ kv.split('=') \n",
    "                        for kv in next_results[1:].split(\"&\") ])\n",
    "        \n",
    "        search_results = twitter_api.search.tweets(**kwargs)\n",
    "        statuses += search_results['statuses']\n",
    "        \n",
    "        if len(statuses) > max_results: \n",
    "            break\n",
    "            \n",
    "    return statuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(message):\n",
    "    \"\"\"\n",
    "    This function takes a string as input, then performs these operations: \n",
    "        - lowercase\n",
    "        - remove URLs\n",
    "        - remove ticker symbols \n",
    "        - removes punctuation\n",
    "        - tokenize by splitting the string on whitespace \n",
    "        - removes any single character tokens\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        message : The text message to be preprocessed.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "        tokens: The preprocessed text into tokens.\n",
    "    \"\"\" \n",
    "    #TODO: Implement \n",
    "    \n",
    "    # Lowercase the twit message\n",
    "    text = message.lower()\n",
    "    \n",
    "    # Replace URLs with a space in the message\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', ' ', text)\n",
    "    \n",
    "    # Replace ticker symbols with a space. The ticker symbols are any stock symbol that starts with $.\n",
    "    text = re.sub(r'[$][A-Za-z][\\S]*', ' ', text)\n",
    "    \n",
    "    # Replace StockTwits usernames with a space. The usernames are any word that starts with @.\n",
    "    text = re.sub(r'[$][A-Za-z][\\S]*', ' ', text)\n",
    "\n",
    "    # Replace everything not a letter with a space\n",
    "    text = re.sub(r'[\\W_]+', ' ', text)\n",
    "    \n",
    "    # Tokenize by splitting the string on whitespace into a list of words\n",
    "    tokens = text.split()\n",
    "\n",
    "    # Lemmatize words using the WordNetLemmatizer. You can ignore any word that is not longer than one character.\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    tokens = [wnl.lemmatize(token) for token in tokens if len(token) > 1]\n",
    "    \n",
    "    return tokens   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_collection(db, coll):\n",
    "    if \"$\" in coll:\n",
    "        coll = coll[1:]\n",
    "    return db[coll]\n",
    "\n",
    "def get_polarity_score(tweets_lst):\n",
    "    polarity_arr = np.zeros(len(tweets_lst))\n",
    "    for i, text in enumerate(tweets_lst):\n",
    "        # Extract the text portion of the tweet\n",
    "\n",
    "        # Measure the polarity of the tweet\n",
    "        polarity = analyzer.polarity_scores(text['text'])    \n",
    "        # Store the normalized, weighted composite score\n",
    "        polarity_arr[i] = polarity['compound']\n",
    "    return polarity_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to search_results DB\n",
    "client = pymongo.MongoClient(host)\n",
    "db = client.search_results\n",
    "# Conenct to twitter\n",
    "twitter_api = oauth_login()\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching twitter for $SPY\n",
      "Searching twitter for $QQQ\n",
      "Searching twitter for $IWM\n",
      "Searching twitter for $DIA\n",
      "Searching twitter for $VTI\n",
      "Searching twitter for $MDY\n",
      "Searching twitter for $DBC\n",
      "Searching twitter for $FEZ\n",
      "Searching twitter for $OEF\n",
      "Searching twitter for $IWF\n",
      "Searching twitter for $IWD\n",
      "Searching twitter for $PFF\n",
      "Searching twitter for $VOO\n",
      "Searching twitter for $IJH\n",
      "Searching twitter for $IWO\n",
      "Searching twitter for $IWN\n",
      "Searching twitter for $ACWI\n",
      "Searching twitter for $IEMG\n"
     ]
    }
   ],
   "source": [
    "# Gather twitter response, insert into DB\n",
    "queries = list(map(lambda x: '$' + x, etfs.keys()))\n",
    "ct = 0\n",
    "for q in queries:\n",
    "    # Create new collection titled the search query\n",
    "    print(f\"Searching twitter for {q}\")\n",
    "    coll = create_collection(db, q)\n",
    "    # Get search results\n",
    "    response = twitter_search(twitter_api, q)\n",
    "    for doc in response:\n",
    "        inserted_id = coll.insert_one(doc).inserted_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tweets for SPY then subset by date posted\n",
    "spy_tweets = [x for x in client['search_results']['SPY'].find({},{\"_id\": 0, \"text\": 1, \"created_at\": 1})]\n",
    "\n",
    "spy_d1 = []\n",
    "spy_d2 = []\n",
    "for i in spy_tweets:\n",
    "    i['created_at'] = datetime.strptime(i['created_at'], '%a %b %d %H:%M:%S %z %Y').strftime(\"%m/%d/%y\")\n",
    "    if i['created_at'] == '04/27/20':\n",
    "        spy_d1.append(i)\n",
    "    elif i['created_at'] == '04/28/20':\n",
    "        spy_d2.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get polarity scores\n",
    "spy_scores_d1 = get_polarity_score(spy_d1)\n",
    "spy_scores_d2 = get_polarity_score(spy_d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPY Sentiment Average April 27th: 0.12\n",
      "SPY Day Change April 27th: +1.44%\n",
      "\n",
      "SPY Sentiment Average April 28th: 0.08\n",
      "SPY Day Change April 28th: -0.47%\n"
     ]
    }
   ],
   "source": [
    "print(f\"SPY Sentiment Average April 27th: {np.average(spy_scores_d1):.2f}\\nSPY Day Change April 27th: +1.44%\")\n",
    "print(f\"\\nSPY Sentiment Average April 28th: {np.average(spy_scores_d2):.2f}\\nSPY Day Change April 28th: -0.47%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tweets for DIA then subset by date posted\n",
    "dia_tweets = [x for x in client['search_results']['DIA'].find({},{\"_id\": 0, \"text\": 1, \"created_at\": 1})]\n",
    "\n",
    "dia_d1 = []\n",
    "dia_d2 = []\n",
    "for i in dia_tweets:\n",
    "    i['created_at'] = datetime.strptime(i['created_at'], '%a %b %d %H:%M:%S %z %Y').strftime(\"%m/%d/%y\")\n",
    "    if i['created_at'] == '04/27/20':\n",
    "        dia_d1.append(i)\n",
    "    elif i['created_at'] == '04/28/20':\n",
    "        dia_d2.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get polarity scores\n",
    "dia_scores_d1 = get_polarity_score(dia_d1)\n",
    "dia_scores_d2 = get_polarity_score(dia_d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIA Sentiment Average April 27th: 0.14\n",
      "DIA Day Change April 27th: +1.47%\n",
      "\n",
      "DIA Sentiment Average April 28th: 0.11\n",
      "DIA Day Change April 28th: -0.12%\n"
     ]
    }
   ],
   "source": [
    "print(f\"DIA Sentiment Average April 27th: {np.average(dia_scores_d1):.2f}\\nDIA Day Change April 27th: +1.47%\")\n",
    "print(f\"\\nDIA Sentiment Average April 28th: {np.average(dia_scores_d2):.2f}\\nDIA Day Change April 28th: -0.12%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
